{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After we separated the labels from GIANT text files using GIANTprocessor, we saved the text file as XXX_citation_text.txt and XXX_citation_label.txt. Then we created a bert_giant dirtectory where we would place all the processed GIANT files which contains only text. We are using those processed GIANT text files to create a BERTcitation-custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_giant/300K_citation_text.txt',\n",
       " 'bert_giant/10K_citation_text.txt',\n",
       " 'bert_giant/1k_citation_text.txt',\n",
       " 'bert_giant/200K_citation_text.txt',\n",
       " 'bert_giant/100K_citation_text.txt',\n",
       " 'bert_giant/22k_citation_text.txt',\n",
       " 'bert_giant/1M_citation_text.txt',\n",
       " 'bert_giant/500_citation_text.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [str(x) for x in Path('./bert_giant').glob('**/*.txt')]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "\n",
    "## During initialization:\n",
    "a) clean_text — cleans text by removing control characters and replacing all whitespace with spaces.\n",
    "b) handle_chinese_chars — whether the tokenizer includes spaces around Chinese characters (if found in the dataset).\n",
    "c) stripe_accents — whether we remove accents, when True this will make é → e, ô → o, etc.\n",
    "d) lowercase — if True the tokenizer will view capital and lowercase characters as equal; A == a, B == b, etc.\n",
    "\n",
    "## During train: \n",
    "a) vocab_size — the number of tokens in our tokenizer. During later tokenization of text, unknown words will be assigned as [UNK] token which is not ideal. \n",
    "We should try to minimize this when possible.\n",
    "\n",
    "b) min_frequency — minimum frequency for a pair of tokens to be merged.\n",
    "c) special_tokens — a list of the special tokens that BERT uses.\n",
    "d) limit_alphabet — maximum number of different characters.\n",
    "e) workpieces_prefix — the prefix added to pieces of words (like ##board in our earlier examples)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    strip_accents=True,\n",
    "    lowercase=True\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=50_000_000, min_frequency=1,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=[\n",
    "                    '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./bert_giant/BERTcitation-vocab.txt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model('./bert_giant', 'BERTcitation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = sorted(glob.glob('/home/mchou001/500citation/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The below block of code will concatanate all the csv files in the downsampled GIANT dataset directories. Each directory contains 219 csv files.\n",
    "\"\"\"\n",
    "newDataset = []\n",
    "for file in dataset_path:\n",
    "    df = pd.read_csv(file, encoding = 'utf-8')\n",
    "    newDataset.append(df)\n",
    "\n",
    "concat_files = pd.concat(newDataset, axis = 0 , ignore_index=True)\n",
    "citationString = concat_files['citationStringAnnotated']\n",
    "citationString.to_csv(\"500_citation.txt\", header=False, index=None)\n",
    "print(len(citationString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "citationList = []\n",
    "for line in citationString:\n",
    "    new_start_tag = \"<citation> \"\n",
    "    new_end_tag = \" </citation>\"\n",
    "    if line.startswith(\"\"):\n",
    "        citation_tag = new_start_tag + line + new_end_tag ## adding a new tag to each citation string to separate from each other\n",
    "        citationList.append(citation_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchou001/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1661: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert_giant/BERTcitation-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(str(citationList), \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding1 = tokenizer.encode(\"1978 New Names in Volume 52 Journal of Helminthology 52 04 December 389 http://dx.doi.org/10.1017/s0022149x00017338 10.1017/s0022149x00017338\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '1978', 'new', 'names', 'in', 'volume', '52', 'journal', 'of', 'helminthology', '52', '04', 'december', '389', 'http', ':', '/', '/', 'dx', '.', 'doi', '.', 'org', '/', '10', '.', '1017', '/', 's0022149x00017338', '10', '.', '1017', '/', 's0022149x00017338', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding2 = tokenizer.encode(\"Matthew W. Veal., Scott A. Shearer., John P. Fulton. Improved Mass Flow Sensing for Yield Monitoring in Grain Combines 2004, Ottawa, Canada August 1 - 4, 2004 American Society of Agricultural and Biological Engineers 2004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'matthew', 'w', '.', 'veal', '.', ',', 'scott', 'a', '.', 'shearer', '.', ',', 'john', 'p', '.', 'fulton', '.', 'improved', 'mass', 'flow', 'sensing', 'for', 'yield', 'monitoring', 'in', 'grain', 'combines', '2004', ',', 'ottawa', ',', 'canada', 'august', '1', '-', '4', ',', '2004', 'american', 'society', 'of', 'agricultural', 'and', 'biological', 'engineers', '2004', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding3 = tokenizer.encode(\"McMillan, Donald C, Naveed Sattar, Dinesh Talwar, Denis St.J O’Reilly, and Colin S McArdle 2000 “Changes in Micronutrient Concentrations Following Anti-Inflammatory Treatment in Patients with Gastrointestinal Cancer.” Nutrition 16 6 Elsevier BV 425–428 10.1016/s0899-9007(00)00270-7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'mcmillan', ',', 'donald', 'c', ',', 'naveed', 'sattar', ',', 'dinesh', 'talwar', ',', 'denis', 'st', '.', 'j', 'o', '’', 'reilly', ',', 'and', 'colin', 's', 'mcardle', '2000', '“', 'changes', 'in', 'micronutrient', 'concentrations', 'following', 'anti', '-', 'inflammatory', 'treatment', 'in', 'patients', 'with', 'gastrointestinal', 'cancer', '.', '”', 'nutrition', '16', '6', 'elsevier', 'bv', '425', '–', '428', '10', '.', '1016', '/', 's0899', '-', '9007', '(', '00', ')', '00270', '-', '7', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2914, 1737, 6807, 1408, 2845, 2546, 1464, 1404, 95243, 2546, 2552, 3315, 6948, 1439, 30, 19, 19, 1441, 18, 1440, 18, 1433, 19, 1400, 18, 2565, 19, 491248, 1400, 18, 2565, 19, 491248, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tokenizer(\"1978 New Names in Volume 52 Journal of Helminthology 52 04 December 389 http://dx.doi.org/10.1017/s0022149x00017338 10.1017/s0022149x00017338\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 1978 new names in volume 52 journal of helminthology 52 04 december 389 http : / / dx. doi. org / 10. 1017 / s0022149x00017338 10. 1017 / s0022149x00017338 [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(embedding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
